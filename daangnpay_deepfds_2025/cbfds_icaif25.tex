%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%


%%
%% The first command in your LaTeX source must be the \documentclass command.
\documentclass[sigconf]{acmart}

 

%% NOTE that a single column version may be required for 
%% submission and peer review. This can be done by changing
%% the \documentclass[...]{acmart} in this template to 
%% \documentclass[manuscript,screen]{acmart}
%% 
%% To ensure 100% compatibility, please check the white list of
%% approved LaTeX packages to be used with the Master Article Template at
%% https://www.acm.org/publications/taps/whitelist-of-latex-packages 
%% before creating your document. The white list page provides 
%% information on how to submit additional LaTeX packages for 
%% review and adoption.
%% Fonts used in the template cannot be substituted; margin 
%% adjustments are not allowed.
%%
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\copyrightyear{2025} 
\acmYear{2025} 
\setcopyright{acmlicensed}\acmConference[ICAIF '25]{6th ACM International Conference on AI in Finance}{November 15--18, 2025}{Singapore}
\acmBooktitle{6th ACM International Conference on AI in Finance (ICAIF '25), November 15--18, 2025, Singapore}
\acmPrice{15.00}
\acmDOI{10.1145/3604237.3626838}
\acmISBN{979-8-4007-0240-2/23/11}




%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.


%% Bibliography style
% \RequirePackage[
%   datamodel=acmdatamodel,
%   style=acmnumeric,
%   ]{biblatex}


%% Declare bibliography sources (one \addbibresource command per source)
% \addbibresource{references.bib} 
% \usepackage{natbib}
% \bibliographystyle{unsrtnat}
\bibliographystyle{ACM-Reference-Format}



\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{In-Context Learning for Smarter Fraud Detection in Remote Flea Market Transactions}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.


\author{Hyunwoo Kim}
\orcid{xxxxxxxx}
% \authornote{These authors contributed equally to this research}
\affiliation{%
  \institution{Danggeun Pay Inc.}
  \state{Seoul}
  \country{Republic of Korea}
}
\email{peter.kim@daangnpay.com}



\author{Hyunmyoung Oh}
\orcid{xxxxxxxx}
% \authornotemark[1]
\affiliation{%
  \institution{Danggeun Pay Inc.}
  \state{Seoul}
  \country{Republic of Korea}
}
\email{hammer@daangnpay.com}



\author{Sunghyon Kyeong}
\orcid{0000-0002-9095-5219}
\authornote{Corresponding author}
\affiliation{%
  \institution{Danggeun Pay Inc.}
  \state{Seoul}
  \country{Republic of Korea}
}
\email{devyn@daangnpay.com}




%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Kim et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
this part is not ready yet.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
  <ccs2012>
    <concept>
      <concept_id>10010147.10010178.10010179</concept_id>
      <concept_desc>Computing methodologies~Natural language processing</concept_desc>
      <concept_significance>500</concept_significance>
    </concept>
    <concept>
      <concept_id>10010147.10010257</concept_id>
      <concept_desc>Computing methodologies~Machine learning</concept_desc>
      <concept_significance>500</concept_significance>
    </concept>
    <concept>
      <concept_id>10002951.10003227.10003228</concept_id>
      <concept_desc>Information systems~Enterprise information systems</concept_desc>
      <concept_significance>500</concept_significance>
    </concept>
  </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Machine learning}
\ccsdesc[500]{Computing methodologies~Natural language processing}
\ccsdesc[500]{Information systems~Enterprise information systems}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{In-context learning, context-based fraud detection, fraud detection, remote secondhand transactions}

% \received{14 July 2025}
% \received[revised]{12 March 2025}
% \received[accepted]{5 June 2025}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle


\section{Introduction}
The global market for secondhand goods has been steadily expanding, driven in large part by the rise of online platforms that facilitate non-face-to-face peer-to-peer transactions. Prominent marketplaces in this domain include Facebook Marketplace (worldwide), Danggeun Market—also known as Karrot—operating in regions such as North America, Korea, and Japan, and Mercari, which is widely used in Japan. These platforms promote the reuse of goods, contributing to environmental sustainability, and attract a growing user base who are motivated by shared social and ecological values.

To ensure secure and convenient transactions for the majority of well-intentioned users, platform providers have implemented protective measures, including escrow-based financial services. Nevertheless, the anonymity and remote nature of these platforms are frequently exploited by malicious actors. For instance, some fraudulent sellers post items at unusually low prices and fail to deliver the products, engaging in what is commonly referred to as merchant fraud. In response, platforms invest substantial effort into detecting and sanctioning such fraudulent activities.

Traditional fraud detection systems have largely relied on rule-based approaches or supervised machine learning (ML) models trained on historical transaction data. Despite ongoing model retraining enabled by MLops platforms, these approaches exhibit clear limitations in rapidly evolving environments like secondhand marketplaces, where contextual factors heavily influence transaction dynamics. In particular, the early detection of novel fraud schemes remains structurally constrained.

Recently, Large Language Models (LLMs) have emerged as a promising alternative to address these limitations. In the context of secondhand trading, LLMs can effectively analyze unstructured textual data—such as listing titles, product descriptions, and seller profiles—to identify suspicious language patterns or detect fraud strategies that resemble previously known cases. Unlike traditional models that rely solely on structured features, LLMs excel at natural language understanding and can capture subtle linguistic cues, inconsistencies in phrasing, and tone variations that might otherwise elude human analysts.

Furthermore, LLMs possess the ability to cross-reference contextual information across multiple transactions. For example, the repeated use of similar phrases, emojis, or urgent language across listings from different user accounts may be linked to a single fraud actor. This capability is significantly enhanced through In-Context Learning (ICL), which enables LLMs to perform fraud detection tasks with minimal examples and without requiring explicit model fine-tuning. ICL is particularly advantageous in scenarios where large-scale labeled datasets are unavailable and where fraud tactics evolve rapidly.

Moreover, the increasing sophistication of fraudsters—who now leverage generative AI to craft convincing phishing messages, fabricate identities, and produce deepfake documents—underscores the urgency for platforms to deploy equally advanced AI-based defense mechanisms. This technological arms race necessitates the adoption of LLM-powered, intelligent fraud detection frameworks.

In this study, we propose a novel fraud detection approach tailored to non-face-to-face secondhand trading environments, leveraging LLM-based In-Context Learning. Specifically, we extract salient features from previously confirmed fraud cases using LLMs to analyze unstructured elements such as listing titles and seller profiles. These features are then compared against ongoing transactions to assess their likelihood of being fraudulent. Finally, we evaluate the effectiveness of our approach in comparison with traditional machine learning-based detection methods to determine its potential performance gains in real-world settings.



% The remainder of this paper is organized as follows: Section~\ref{korkanti2024} presents a comprehensive review of the relevant literature. In Section~\ref{methodology}, the proposed methodology is outlined, including a description of the Korean baseline model and the fine-tuning process using instruct datasets. Section~\ref{evaluation_metrics} describes the evaluation framework of the fine-tuned Korean PLMs. The experimental setups and results are then presented in Sections~\ref{experiments} and \ref{results}, respectively. Finally, Section~\ref{conclusions} summarizes the findings.



\section{Related Works}\label{related_work}
\subsection{ML-Based Fraud Detection}
A wide range of studies in both industry and academia have sought to advance techniques for financial fraud detection. One line of research focuses on representing transaction histories between bank accounts as graphs, enabling the development of graph-based fraud detection models that significantly outperform traditional baselines in terms of F1 score performance~\cite{lin2024graphtransformer, yoo2023medicare}.

Simultaneously, increasing attention has been paid to fraud in peer-to-peer transactions within online marketplaces, where financial transactions often accompany interpersonal exchanges. A prominent example is merchant fraud, in which a scammer lists trending products at unusually low prices, receives payment, and fails to deliver the goods. This type of fraud is especially prevalent in remote secondhand platforms. Some studies have addressed this issue by analyzing fraudulent seller accounts and building machine learning-based detection models using features derived from transaction histories and product listings~\cite{hasan2022ecommerce, renjith2018}.

\subsection{LLM-Based Fraud Detection}  
With the advent of large language models (LLMs), researchers and practitioners have actively explored their potential for financial fraud detection. Traditional methods—such as logistic regression, random forests, and neural networks—have long been applied to detect fraud (e.g., in credit card transactions), but these models face limitations when dealing with highly imbalanced datasets and evolving fraud patterns~\cite{yu2024card_fds}.

Recent studies suggest that Transformer-based LLMs are better suited for capturing long-range dependencies and subtle correlations in transaction data, leading to improved detection performance~\cite{chen2021pareto, liu2019stockline}. For example, Yu et al. (2024) demonstrate that Transformer-based models outperform conventional machine learning approaches in terms of accuracy and are particularly effective at identifying rare fraudulent cases~\cite{yu2024card_fds, lyu2023attention}. The pretraining of LLMs on vast corpora enables them to form a form of commonsense understanding of sequences, which can be further enhanced through retrieval-augmented generation (RAG) methods to boost detection capabilities~\cite{pandey2024rag}.

Moreover, LLMs have proven useful in processing unstructured data alongside structured transactional features. Butler (2025) highlights that LLMs can detect fraud-indicative language and anomalies in textual sources such as transaction notes, emails, and chat logs. This capacity allows them to surface social engineering attempts or abnormal phrasing in online interactions—types of fraud that often evade detection by traditional rule-based or statistical systems.


\subsection{In-Context Learning for Fraud Pattern Recognition}
In-context learning (ICL) has emerged as a powerful paradigm that enables LLMs to perform tasks without explicit fine-tuning. Introduced by Brown et al. (2020) with the release of GPT-3, ICL allows a model to generalize to new tasks using only a prompt containing a few labeled examples~\cite{brown2020llm_fewshot}. This characteristic makes ICL particularly well-suited for fraud detection scenarios, where only a small number of examples of emerging fraud types may be available.

Through ICL, LLMs can implicitly learn patterns from a few in-context examples and adapt to new fraud types in real time. Liu et al. (2024) apply this concept to graph-based anomaly detection, using a handful of normal nodes as context to identify outliers in unseen graphs without additional training~\cite{liu2024anomaly}. Similarly, Bhattacharya et al. (2025) propose a system that converts structured transaction features (e.g., amount, location, device information) into natural language descriptions and feeds them into an LLM along with a few labeled examples, enabling accurate classification of novel transactions as fraudulent or legitimate~\cite{bhattacharya2024fraud}.



\section{Datasets}\label{datasets}

\begin{figure}[t!]
  \centering
  \includegraphics[width=0.48\textwidth]{figures/fig_dataset.png}
  \caption{Overview of the sampled dataset with fraud label distribution}
  \label{fig_dataset}
\end{figure}


This study leverages proprietary real-world transaction data provided by Danggeun Pay Inc., a financial technology company that operates the official payment infrastructure for Danggeun Market Inc.—a widely used local community platform in South Korea. The platform supports a variety of services, including secondhand goods trading, real estate listings, part-time job postings, and more. Within this ecosystem, Danggeun Pay facilitates peer-to-peer (P2P) payments, enabling the collection of fine-grained transactional records that are particularly rich in behavioral signals relevant to fraud detection.

The dataset comprises transaction-level records labeled as either fraudulent or normal. Each transaction is augmented with accompanying listing metadata as well as detailed behavioral features extracted from the seller's historical activity. The dataset was curated for the express purpose of facilitating machine learning research on fraud detection in P2P commerce and offers a comprehensive foundation for studying behavioral patterns in online trust-mediated environments.


\begin{table}[b!]
  \centering
  \begin{tabular*}{\columnwidth}{l@{\extracolsep{\fill}}cc}
  \hline
  \textbf{Category} & \textbf{Fraud} & \textbf{Normal} \\
  \hline
  Tickets & 305 & 464 \\
  Fashion \& Miscellaneous & 25 & 184 \\
  Baby \& Kids & 78 & 37 \\
  Electronics & 37 & 84 \\
  Sports & 12 & 32 \\
  Others & 13 & 99 \\
  \hline
  \end{tabular*}
  \caption{Category-wise distribution of the dataset by fraud label}
  \label{tab_category_distribution}
\end{table}



\begin{figure*}[t!]
  \centering
  \includegraphics[width=0.98\textwidth]{./figures/fig_process.png}
  \caption{Overview of the experimental processes. A) One-step inference approach. B) Two-step inference approach.}
  \label{fig_process}
\end{figure*}



\subsection{Training and Test Split}
The dataset includes transactions conducted over a two-month period, from April to May 2025. To ensure balanced model training and fair evaluation across fraud classes, stratified sampling based on ground-truth fraud labels was applied. As shown in Fig.~\ref{fig_dataset}A, a total of 1,370 transactions were selected, with a class distribution ratio of approximately 1:2 (fraudulent to normal transactions), providing a sufficiently diverse dataset for evaluating fraud detection performance.

The dataset was temporally partitioned based on the transaction date. Transactions that occurred in April 2025 were designated as the training dataset (430 legitimate cases; 204 fraudulent cases), while those from May 2025 were allocated to the test dataset (470 legitimate cases; 266 fraudulent cases). The training dataset was not primarily used for direct model training but rather for extracting recent fraud cases during the experimental process. The test dataset was utilized to evaluate the performance of the proposed fraud detection methods.



\subsection{Category-Wise Distribution}
The sampled transactions span seven major product categories, each exhibiting distinct fraud risk profiles as shown in Fig.~\ref{fig_dataset}B. Table~\ref{tab_category_distribution} summarizes the distribution of fraudulent and legitimate transactions across these categories.

Notably, categories with high liquidity and resale value—such as tickets—exhibit a disproportionately high rate of fraudulent activity. This heterogeneity underscores the importance of incorporating category-specific behavioral patterns into fraud detection models.



\subsection{Feature Overview}\label{feature_overview}
Each transaction instance in the dataset is represented by a set of features grouped into four key dimensions:

\begin{itemize}
    \item \textbf{Listing Metadata}: This feature includes the listing title (\texttt{title}), listed price (\texttt{price}), and product category (\texttt{category}).
    \item \textbf{Transaction Details}: This feature captures transaction timestamp (\texttt{tx\_dttm}) and transaction amount (\texttt{tx\_amt}).
    \item \textbf{Seller Profile}: This feature includes demographic and account-level attributes such as seller age (\texttt{seller\_age}) and account tenure in days (\texttt{seller\_account\_tenure}).
    \item \textbf{Recent Seller Activity}: This feature summarizes behavioral signals over a 24-hour window preceding the listing. This includes the number of prior transactions (\texttt{recent\_tx\_cnt}), cumulative transaction amount (\texttt{recent\_tx\_amt\_sum}), and number of unique counterparties (\texttt{recent\_unique\_buyers}).
\end{itemize}

These feature groups collectively capture both static attributes and dynamic behavioral cues, facilitating a comprehensive analysis of user behavior for fraud detection.




\begin{figure}[b!]
  \centering
  \includegraphics[width=0.48\textwidth]{./figures/fig_prompt_1step.png}
  \caption{Partial description for one-step approach. The red text enclosed in double curly brackets denotes dynamic input parameters that were updated at each LLM invocation.}
\label{fig_prompt_1step}
\end{figure}



\section{Experiments}

This study investigates whether a large language model (LLM) can accurately assess the likelihood of fraud in financial transactions, given rich contextual information specific to peer-to-peer remote secondhand marketplaces. To this end, we designed a series of experiments aimed at evaluating the LLM’s ability to make context-aware inferences about the legitimacy of each transaction event.

Two primary inference approaches were explored as illustrated in Fig.~\ref{fig_process}. In the one-step inference method, prompts were constructed to directly assess whether a given transaction was fraudulent. Each prompt embedded comprehensive contextual information about the target transaction along with recent fraud examples, enabling the LLM to leverage both transaction-specific features and patterns from past fraud cases.

In the two-step inference method, the fraud detection process was decomposed into two stages. In the first stage, the LLM was prompted with recent fraud examples to identify distinct fraud clusters and extract representative features for each group. In the second stage, these extracted fraud patterns were combined with the full contextual information of the target transaction in a new prompt, allowing the LLM to determine whether the transaction aligned with any known fraud patterns.


\begin{figure*}[t!]
  \centering
  \includegraphics[width=0.98\textwidth]{./figures/fig_prompt_2step.png}
  \caption{Illustrative description for Two-step prompts. A) First step prompt to cluster frauds and extract features. B) Second step prompt to determine whether the current transaction is fraudulent. The red text enclosed in double curly brackets denotes dynamic input parameters that were updated at each LLM invocation.}
  \label{fig_prompt_2step}
\end{figure*}


\subsection{Proprietary LLM}\label{proprietary_llm}
We conducted experiments to evaluate whether state-of-the-art proprietary LLMs can assess the likelihood of fraud by leveraging in-context learning, using real-world examples of recent fraudulent transactions along with rich contextual information surrounding the current transaction. Specifically, we investigated the extent to which each model could comprehend the nuances of financial fraud scenarios and accurately infer fraud likelihood in a context-sensitive manner.
Three leading proprietary LLMs were selected for this evaluation: OpenAI's GPT-4.1, Google's Gemini 2.5 Flash, and Anthropic's Claude Sonnet 4.0. These models were tested under identical prompt structures and inference conditions to ensure a fair comparison of their reasoning capabilities and context sensitivity in the domain of peer-to-peer financial fraud detection.



\subsection{One-Step Fraud Inference}
In the one-step fraud inference approach, the prompt was constructed by inserting comprehensive contextual information about the transaction under evaluation, recent fraud examples, and a description of high-risk fraud indicators. The large language model (LLM) was then prompted to determine whether or not the transaction under evaluation is fraud. The prompt format used in the one-step fraud inference experiment is illustrated in Fig.~\ref{fig_prompt_1step}. In this figure, the red text enclosed in double curly brackets denotes dynamic input parameters that were updated at each LLM invocation.

The \texttt{\{\{current\_transaction\}\}} slot was filled with full contextual information about the transaction, incorporating all features described in Section~\ref{feature_overview}. The \texttt{\{\{recent\_fraud\_examples\}\}} slot included listing attributes and seller profile information from past transactions that had been labeled as fraudulent.

To investigate the impact of the number of few-shot examples on inference performance, we varied the number of recent fraud examples (N) inserted into the prompt. Specifically, for each target transaction, we extracted the most recent N fraud cases that occurred within the 24-hour period preceding it. 

The experiments were independently conducted for each of the three proprietary LLMs—GPT-4.1, Gemini 2.5 Flash, and Claude Sonnet 4.0—by repeatedly applying the same protocol across varying values of N (10, 30, 50, 70, 90). This ensured a consistent and comparative evaluation of the one-step inference performance across all models under different prompt sizes.






\subsection{Two-Step Fraud Inference}

In the two-step fraud inference approach, the evaluation of a target transaction's likelihood of being fraudulent was conducted through a two-stage invocation of the LLM (Fig.~\ref{fig_process}B).
In the first step, the LLM was prompted with recent fraud examples to perform fraud clustering and extract representative features for each identified cluster (see the descriptive LLM prompt in Fig.~\ref{fig_prompt_2step}A). The fraud features generated in this step—derived in a data-driven manner—were subsequently used as input for the second LLM invocation.

In the second step, the LLM was presented with a new prompt (Fig.~\ref{fig_prompt_2step}B) that incorporated both the fraud patterns extracted in the first step and the full contextual information of the target transaction as described in Section~\ref{feature_overview}. Based on this prompt, the LLM was tasked with assessing whether the given transaction exhibited fraudulent characteristics or not.

As in the one-step inference approach, we investigated how the number of few-shot examples influenced both the quality of feature extraction and the performance of fraud inference. For each transaction, we selected the most recent N fraud cases that occurred within a 24-hour window prior to the transaction. 

The same two-step inference procedure was independently applied to all three proprietary LLMs—GPT-4.1, Gemini 2.5 Flash, and Claude Sonnet 4.0—repeatedly across different values of N (10, 30, 50, 70, and 90), enabling a model-by-model comparison under identical experimental conditions.





\subsection{Evaluation Metrics}\label{evaluation_metrics}
To comprehensively assess the performance of the proposed fraud inference approaches, we report three widely adopted evaluation metrics: precision, recall, and F1-score. These metrics are particularly important in real-world fraud detection scenarios, where both false positives (misclassifying a legitimate transaction as fraudulent) and false negatives (failing to detect an actual fraud) entail significant practical consequences.
Precision quantifies the proportion of transactions predicted as fraudulent that are indeed fraudulent. It reflects the model's effectiveness in minimizing false alarms and is particularly valuable when the cost of incorrectly flagging legitimate users is high.
\begin{equation}
 \text{Precision} = \frac{\text{True Positive}}{\text{True Positive} + \text{False Positive}}
 \end{equation}
Recall measures the proportion of actual fraudulent transactions that are correctly identified by the model. It captures the model's sensitivity to fraud cases and is especially critical when the cost of missing fraudulent behavior is high.
\begin{equation}
 \text{Recall} = \frac{\text{True Positive}}{\text{True Positive} + \text{False Negative}}
 \end{equation}
F1-score is the harmonic mean of precision and recall, providing a balanced metric that accounts for both types of classification errors. It is particularly suitable when neither precision nor recall can be compromised, as is often the case in fraud detection systems.
\begin{equation}
 \text{F1-score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
 \end{equation}




\section{Experimental Results}\label{results}
this part is not ready yet.





\subsection{Performance of fine-tuning models}
this part is not ready yet.







\section{Conclusions}\label{conclusions}
this part is not ready yet.



%% Print the bibliography
% \printbibliography

\bibliography{references.bib} 



\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
