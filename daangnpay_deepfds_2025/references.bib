@inproceedings{korkanti2024,
      author={Korkanti, Sukanth},
      title={Enhancing Financial Fraud Detection Using {LLMs} and Advanced Data Analytics},
      booktitle={2024 2nd International Conference on Self Sustainable Artificial Intelligence Systems (ICSSAS)},
      year={2024},
      pages={1328--1334},
      doi={10.1109/ICSSAS64001.2024.10760895}
}


@article{renjith2018,
  author={Renjith, Shini},
  title={Detection of Fraudulent Sellers in Online Marketplaces using Support Vector Machine Approach},
  journal={International Journal of Engineering Trends and Technology (IJETT)},
  volume={57},
  number={1},
  year={2018},
  month={March},
  pages={48--53},
  doi={10.14445/22315381/IJETT-V57P210}
}

@inproceedings{yu2024card_fds,
      author={Yu, Chang and Xu, Yongshun and Cao, Jin and Zhang, Ye and Jin, Yixin and Zhu, Mengran},
      title={Credit Card Fraud Detection Using Advanced Transformer Model},
      booktitle={2024 IEEE International Conference on Metaverse Computing, Networking, and Applications (MetaCom)},
      year={2024},
      pages={343--350},
      doi={10.1109/MetaCom62920.2024.00064}
}

@misc{chen2021pareto,
      author={Chen, Zhengyu and Ge, Jixie and Zhan, Heshen and Huang, Siteng and Wang, Donglin},
      title={Pareto Self-Supervised Training for Few-Shot Learning},
      year={2021},
      eprint={2104.07841},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2104.07841}
}

@misc{lyu2023attention,
      author={Lyu, Weimin and Zheng, Songzhu and Pang, Lu and Ling, Haibin and Chen, Chao},
      title={Attention-Enhancing Backdoor Attacks Against {BERT}-based Models},
      year={2023},
      eprint={2310.14480},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2310.14480}
}

@article{liu2019stockline,
      author={Liu, Xiaopeng and Liu, Yan and Zhang, Meng and Chen, Xianzhong and Li, Jiangyun},
      title={Improving Stockline Detection of Radar Sensor Array Systems in Blast Furnaces Using a Novel Encoder--Decoder Architecture},
      journal={Sensors},
      volume={19},
      number={16},
      year={2019},
      pages={3470},
      doi={10.3390/s19163470},
      url={https://www.mdpi.com/1424-8220/19/16/3470}
}

@inproceedings{pandey2024rag,
      author = {Pandey, Anubha},
      title = {Retrieval Augmented Fraud Detection},
      year = {2024},
      isbn = {9798400710810},
      publisher = {Association for Computing Machinery},
      address = {New York, NY, USA},
      url = {https://doi.org/10.1145/3677052.3698692},
      doi = {10.1145/3677052.3698692},
      abstract = {Fraud detection in the financial landscape poses unique multifaceted challenges, such as extreme class imbalance, high feature cardinality, adversarial dynamics, and non-IID data distributions. While traditional machine learning methods [1] and recent deep learning approaches [21, 22, 37] have made strides in tackling these issues, significant room for improvement remains. In this paper, we propose Retrieval Augmented Fraud Detection (RAFD), a novel approach that builds upon the success of self-supervised representation learning methods like SAINT [32] and incorporates principles from Retrieval Augmented Classification (RAC) [23] to enhance fraud detection capabilities. RAFD utilizes a pre-trained SAINT encoder augmented with retrieval, integration, and predictor modules, jointly trained to dynamically leverage similar instances for each input sample. This approach enables context-rich decision-making, adaptive sampling, and improved generalization, particularly for the underrepresented fraud class. By combining instance-based learning with deep classification techniques, RAFD offers a more robust and adaptable framework for tackling the core challenges of fraud detection. Experimental results demonstrate RAFD’s superior performance over existing methods, particularly in handling class imbalance and detecting nuanced fraud patterns present in the out-of-time test set. This research contributes a significant advancement in fraud detection methodology, with potential implications for enhancing financial security and maintaining the integrity of digital financial ecosystems.},
      booktitle = {Proceedings of the 5th ACM International Conference on AI in Finance},
      pages = {328-335},
      numpages = {8},
      keywords = {Credit Card Fraud Detection, Imbalanced Classification, Retrieval Augmented Classification, Self-supervised Learning, Transformers},
      location = {Brooklyn, NY, USA},
      series = {ICAIF '24}
}

@inproceedings{lin2024graphtransformer,
      author = {Lin, Junhong and Guo, Xiaojie and Zhu, Yada and Mitchell, Samuel and Altman, Erik and Shun, Julian},
      title = {FraudGT: A Simple, Effective, and Efficient Graph Transformer for Financial Fraud Detection},
      year = {2024},
      isbn = {9798400710810},
      publisher = {Association for Computing Machinery},
      address = {New York, NY, USA},
      url = {https://doi.org/10.1145/3677052.3698648},
      doi = {10.1145/3677052.3698648},
      abstract = {Fraud detection plays a crucial role in the financial industry, preventing significant financial losses. Traditional rule-based systems and manual audits often struggle with the evolving nature of fraud schemes and the vast volume of transactions. Recent advances in machine learning, particularly graph neural networks (GNNs), have shown promise in addressing these challenges. However, GNNs still face limitations in learning intricate patterns, effectively utilizing edge attributes, and maintaining efficiency on large financial graphs. To address these limitations, we introduce FraudGT, a simple, effective, and efficient graph transformer (GT) model specifically designed for fraud detection in financial transaction graphs. FraudGT leverages edge-based message passing gates and an edge attribute-based attention bias to enhance its ability to discern important transactional features and differentiate between normal and fraudulent transactions. Our model achieves state-of-the-art performance in detecting fraudulent activities while demonstrating high throughput and significantly lower latency compared to existing methods. We validate the effectiveness of FraudGT through extensive experiments on multiple large-scale synthetic financial datasets. FraudGT consistently outperforms other models, achieving 7.8–17.8\% higher F1 scores, while delivering an average of 2.4 \texttimes{} greater throughput and reduced latency. Our code and datasets are available at https://github.com/junhongmit/FraudGT.},
      booktitle = {Proceedings of the 5th ACM International Conference on AI in Finance},
      pages = {292-300},
      numpages = {9},
      keywords = {Financial transaction networks, fraud detection, graph learning, graph neural networks, graph transformers},
      location = {Brooklyn, NY, USA},
      series = {ICAIF '24}
}


@article{yoo2023medicare,
      author={Yoo, Yeeun and Shin, Jinho and Kyeong, Sunghyon},
      journal={IEEE Access}, 
      title={Medicare Fraud Detection Using Graph Analysis: A Comparative Study of Machine Learning and Graph Neural Networks}, 
      year={2023},
      volume={11},
      number={},
      pages={88278-88294},
      keywords={Graph neural networks;Medical services;Biological system modeling;Machine learning;Insurance;Medical diagnostic imaging;Fraud;Graph neural network;graph centrality measure;machine learning;medicare fraud detection},
      doi={10.1109/ACCESS.2023.3305962}
}




@inproceedings{hasan2022ecommerce,
      author={Hasan, Fahim and Mondal, Sourov Kumar and Kabir, Md. Rayhan and Al Mamun, Md Abdullah and Rahman, Nur Salman and Hossen, Md. Sagar},
      booktitle={2022 7th International Conference on Communication and Electronics Systems (ICCES)}, 
      title={E-commerce Merchant Fraud Detection using Machine Learning Approach}, 
      year={2022},
      volume={},
      number={},
      pages={1123-1127},
      keywords={},
      doi={10.1109/ICCES54183.2022.9835868}
}


@article{bhattacharya2024fraud,
      title = {Accounting fraud detection using contextual language learning},
      journal = {International Journal of Accounting Information Systems},
      volume = {53},
      pages = {100682},
      year = {2024},
      issn = {1467-0895},
      doi = {https://doi.org/10.1016/j.accinf.2024.100682},
      url = {https://www.sciencedirect.com/science/article/pii/S1467089524000150},
      author = {Indranil Bhattacharya and Ana Mickovic},
      keywords = {Accounting fraud detection, Natural Language Processing, BERT, Information Retrieval},
      abstract = {Accounting fraud is a widespread problem that causes significant damage in the economic market. Detection and investigation of fraudulent firms require a large amount of time, money, and effort for corporate monitors and regulators. In this study, we explore how textual contents from financial reports help in detecting accounting fraud. Pre-trained contextual language learning models, such as BERT, have significantly advanced natural language processing in recent years. We fine-tune the BERT model on Management Discussion and Analysis (MD&A) sections of annual 10-K reports from the Securities and Exchange Commission (SEC) database. Our final model outperforms the textual benchmark model and the quantitative benchmark model from the previous literature by 15% and 12%, respectively. Further, our model identifies five times more fraudulent firm-year observations than the textual benchmark by investigating the same number of firms, and three times more than the quantitative benchmark. Optimizing this investigation process, where more fraudulent observations are detected in the same size of the investigation sample, would be of great economic significance for regulators, investors, financial analysts, and auditors.}
}

@misc{brown2020llm_fewshot,
  author={Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  title={Language Models are Few-Shot Learners},
  year={2020},
  eprint={2005.14165},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2005.14165}
}

@misc{liu2024anomaly,
  author={Liu, Shuo and Yao, Di and Fang, Lanting and Li, Zhetao and Li, Wenbin and Feng, Kaiyu and Ji, XiaoWen and Bi, Jingping},
  title={{AnomalyLLM}: Few-shot Anomaly Edge Detection for Dynamic Graphs using Large Language Models},
  year={2024},
  eprint={2405.07626},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2405.07626}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
